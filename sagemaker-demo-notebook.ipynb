{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sagemaker-demo-notebook\n",
    "\n",
    "This notebook is an interactive companion to the article. In it we will do the following:\n",
    "\n",
    "* Build a machine learning model image and store it on ECR, Amazon's container registry service.\n",
    "* Train a machine learning model based on the image we just pushed.\n",
    "* Deploy that model to a web endpoint.\n",
    "* Deploy an arbitrary Sagemaker-complaint model artifact to a web endpoint.\n",
    "* Perform a batch classification job using a SageMaker-compliant model artifact (unfinished?).\n",
    "\n",
    "You may run this notebook either locally or in an AWS SageMaker instance.\n",
    "\n",
    "If you are running locally, make sure that the account you are running this notebook under has all of the necessary permissions: `S3ReadOnlyAccess`, `SagemakerFullAccess`, `iam:GetRole`, and `ECRFullAccess`.\n",
    "\n",
    "If you are running on AWS SageMaker, make sure that the role you pass to the notebook instance has all of these permissions available. Note that the default SageMaker execution context is **not** enough; it has the first permissions in the list above but not the latter two. You need to attach those permissions to the instance yourself.\n",
    "\n",
    "\n",
    "## Getting the code\n",
    "\n",
    "We start by downloading the code from [its repository](https://github.com/ResidentMario/quilt-sagemaker-demo) on GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'quilt-sagemaker-demo'...\n",
      "remote: Enumerating objects: 95, done.\u001b[K\n",
      "remote: Counting objects: 100% (95/95), done.\u001b[K\n",
      "remote: Compressing objects: 100% (66/66), done.\u001b[K\n",
      "remote: Total 95 (delta 48), reused 72 (delta 25), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (95/95), done.\n"
     ]
    }
   ],
   "source": [
    "!rm -rf quilt-sagemaker-demo > /dev/null 2>&1\n",
    "!git clone https://github.com/ResidentMario/quilt-sagemaker-demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "app.py       Dockerfile             requirements.txt\r\n",
      "build.ipynb  health-check-data.csv  \u001b[0m\u001b[01;32mrun.sh\u001b[0m*\r\n"
     ]
    }
   ],
   "source": [
    "%ls quilt-sagemaker-demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The files are:\n",
    "* `build.ipynb` &mdash; A Jupyter notebook that walks through building and training a model for classifying clothing that is based on the Fashion MNIST dataset.\n",
    "* `app.py` &mdash; A simple `flask` app that serves a SageMaker-compliant model-as-an-app.\n",
    "* `health-check-data.csv` &mdash; A small sample dataset used to ping the web service for health checks.\n",
    "* `Dockerfile` &mdash; A Dockerfile that builds an image suitable for distribution on SageMaker.\n",
    "* `run.sh` &mdash; The image runtime entrypoint.\n",
    "* `requirements.txt` &mdash; A list of dependencies necesssary for building or running the model (locally or remotely).\n",
    "\n",
    "...and this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pusing the container\n",
    "\n",
    "The following shell script, inlined in this notebook, builds the Docker image we've imported and stores it in ECR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n",
      "Sending build context to Docker daemon  5.806MB\r",
      "\r\n",
      "Step 1/15 : FROM python:3.6\n",
      " ---> 55fb8aca33df\n",
      "Step 2/15 : RUN [\"mkdir\", \"app\"]\n",
      " ---> Using cache\n",
      " ---> 820a184440b0\n",
      "Step 3/15 : WORKDIR \"app\"\n",
      " ---> Using cache\n",
      " ---> f1212684fe5c\n",
      "Step 4/15 : COPY \"requirements.txt\" .\n",
      " ---> Using cache\n",
      " ---> 9f5df3de1353\n",
      "Step 5/15 : RUN [\"pip\", \"install\", \"-r\", \"requirements.txt\"]\n",
      " ---> Using cache\n",
      " ---> ef8ee61411cd\n",
      "Step 6/15 : COPY \"app.py\" .\n",
      " ---> d1e09b8de2ca\n",
      "Step 7/15 : COPY \"run.sh\" .\n",
      " ---> f814dd9b407b\n",
      "Step 8/15 : COPY \"build.ipynb\" .\n",
      " ---> 483adaf0e2c9\n",
      "Step 9/15 : COPY \"catalog-screencap.png\" .\n",
      " ---> 8655d266443b\n",
      "Step 10/15 : COPY \"health-check-data.csv\" .\n",
      " ---> 75f7bae55dc5\n",
      "Step 11/15 : ENV FLASK_APP app.py\n",
      " ---> Running in 2f7a7b2f3710\n",
      "Removing intermediate container 2f7a7b2f3710\n",
      " ---> 90038dd03705\n",
      "Step 12/15 : RUN [\"chmod\", \"+x\", \"./run.sh\"]\n",
      " ---> Running in 83505dab532c\n",
      "Removing intermediate container 83505dab532c\n",
      " ---> 05ce05a4a6cc\n",
      "Step 13/15 : EXPOSE 5000\n",
      " ---> Running in 3730cff02ba2\n",
      "Removing intermediate container 3730cff02ba2\n",
      " ---> a0e49e55ad3d\n",
      "Step 14/15 : ENTRYPOINT [\"./run.sh\"]\n",
      " ---> Running in 9bc87e7b8573\n",
      "Removing intermediate container 9bc87e7b8573\n",
      " ---> f9eed0e66dd5\n",
      "Step 15/15 : CMD [\"build\"]\n",
      " ---> Running in e8993f02253a\n",
      "Removing intermediate container e8993f02253a\n",
      " ---> 2af866fe6681\n",
      "Successfully built 2af866fe6681\n",
      "Successfully tagged quiltdata/sagemaker-demo:latest\n",
      "The push refers to repository [730278974607.dkr.ecr.us-east-1.amazonaws.com/quiltdata/sagemaker-demo]\n",
      "ae5fbe8f5fba: Preparing\n",
      "b9f5f71239a6: Preparing\n",
      "fe089c15df0a: Preparing\n",
      "cd52a7da1d79: Preparing\n",
      "ae5fbe8f5fba: Preparing\n",
      "884a783a16c6: Preparing\n",
      "df546bc243f7: Preparing\n",
      "0a85eaeb04e4: Preparing\n",
      "0981a0c176c7: Preparing\n",
      "91e3171ff3f4: Preparing\n",
      "8f30c8a64fbc: Preparing\n",
      "04d9107330ad: Preparing\n",
      "b82de10e4f8e: Preparing\n",
      "1a36262221c3: Preparing\n",
      "d2217ead3a1c: Preparing\n",
      "b53b57a50746: Preparing\n",
      "d2518892581f: Preparing\n",
      "c581f4ede92d: Preparing\n",
      "04d9107330ad: Waiting\n",
      "df546bc243f7: Waiting\n",
      "b82de10e4f8e: Waiting\n",
      "0a85eaeb04e4: Waiting\n",
      "0981a0c176c7: Waiting\n",
      "91e3171ff3f4: Waiting\n",
      "8f30c8a64fbc: Waiting\n",
      "d2217ead3a1c: Waiting\n",
      "b53b57a50746: Waiting\n",
      "d2518892581f: Waiting\n",
      "c581f4ede92d: Waiting\n",
      "1a36262221c3: Waiting\n",
      "ae5fbe8f5fba: Pushed\n",
      "884a783a16c6: Pushed\n",
      "fe089c15df0a: Pushed\n",
      "b9f5f71239a6: Pushed\n",
      "cd52a7da1d79: Pushed\n",
      "0981a0c176c7: Pushed\n",
      "0a85eaeb04e4: Pushed\n",
      "8f30c8a64fbc: Pushed\n",
      "91e3171ff3f4: Pushed\n",
      "b82de10e4f8e: Pushed\n",
      "04d9107330ad: Pushed\n",
      "b53b57a50746: Pushed\n",
      "d2518892581f: Pushed\n",
      "d2217ead3a1c: Pushed\n",
      "c581f4ede92d: Pushed\n",
      "1a36262221c3: Pushed\n",
      "df546bc243f7: Pushed\n",
      "latest: digest: sha256:37b30d681ece190318c8c6bef805916ecbcd9df4ed931ff7594d97db7601871a size: 4095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "# construct the ECR name.\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "region=$(aws configure get region)\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/quiltdata/sagemaker-demo:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "# The pipe trick redirects stderr to stdout and passes it /dev/null.\n",
    "# It's just there to silence the error.\n",
    "aws ecr describe-repositories --repository-names \"quiltdata/sagemaker-demo\" > /dev/null 2>&1\n",
    "\n",
    "# Check the error code, if it's non-zero then know we threw an error and no repo exists\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"quiltdata/sagemaker-demo\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "$(aws ecr get-login --region ${region} --no-include-email)\n",
    "\n",
    "# Build the docker image, tag it with the full name, and push it to ECR\n",
    "docker build  -t \"quiltdata/sagemaker-demo\" quilt-sagemaker-demo/\n",
    "docker tag \"quiltdata/sagemaker-demo\" ${fullname}\n",
    "\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a model\n",
    "\n",
    "We use `sagemaker.estimator.Estimator` object to perform model training.\n",
    "\n",
    "Note that the `Estimator` object is parameterized with the image ARN (resource name), a role and session (passed down from the role executing this notebook instance), an instance and instance count, and an output path.\n",
    "\n",
    "The `output_path` is an interesting case. The default behavior of the various algorithms that SageMaker comes packaged with is to output a `*.tar.gz` model artifact into an S3 bucket, and this is a design pattern you are encouraged to use when using a custom image (as well) by e.g. the presence of this argument.\n",
    "\n",
    "Our image serializes model objects itself instead of relying on SageMaker to do it for us, rendering this argument useless. However it's not wise to omit it as SageMaker will automatically create a fresh run-dependent bucket for you if you do...\n",
    "\n",
    "**User note**: you should change `output_path` in the code cell that follows to any random S3 bucket that you own or that hasn't been claimed yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import re\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "import sagemaker as sage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this line of code require additional iam:GetRole permissions.\n",
    "role = get_execution_role()\n",
    "\n",
    "sess = sage.Session()\n",
    "\n",
    "account = sess.boto_session.client('sts').get_caller_identity()['Account']\n",
    "region = sess.boto_session.region_name\n",
    "image = '{}.dkr.ecr.{}.amazonaws.com/quiltdata/sagemaker-demo'.format(account, region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is defined training is performed via `Esimator.fit`, mimicking the `scikit-learn` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sagemaker-demo-2019-01-16-23-24-48-787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-16 23:24:48 Starting - Starting the training job...\n",
      "2019-01-16 23:24:50 Starting - Launching requested ML instances......\n",
      "2019-01-16 23:25:55 Starting - Preparing the instances for training...\n",
      "2019-01-16 23:26:43 Downloading - Downloading input data\n",
      "2019-01-16 23:26:43 Training - Downloading the training image.....\n",
      "\u001b[31m[NbConvertApp] Converting notebook build.ipynb to notebook\u001b[0m\n",
      "\u001b[31m[NbConvertApp] Executing notebook with kernel: python3\u001b[0m\n",
      "\n",
      "2019-01-16 23:27:24 Training - Training image download completed. Training in progress.\u001b[31m2019-01-16 23:27:51.608769: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\u001b[0m\n",
      "\n",
      "2019-01-16 23:28:20 Uploading - Uploading generated training model\u001b[31m[NbConvertApp] Writing 397368 bytes to build.ipynb\u001b[0m\n",
      "\n",
      "2019-01-16 23:28:25 Completed - Training job completed\n",
      "Billable seconds: 110\n"
     ]
    }
   ],
   "source": [
    "clf = sage.estimator.Estimator(image,\n",
    "                               role, 1, 'ml.c4.2xlarge',\n",
    "                               output_path=\"s3://alpha-quilt-storage/junk\",\n",
    "                               sagemaker_session=sess)\n",
    "\n",
    "clf.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this code block trains out model and deposits it in a `clf.tar.gz` file in an S3 bucket somewhere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying a model\n",
    "\n",
    "### Deploy a fitted model as an endpoint\n",
    "\n",
    "If we handle writing model artifacts ourselves directly in the image, it becomes necessary to overwrite the `model_data` class property as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sage.estimator.Estimator.model_data =\\\n",
    "    \"s3://alpha-quilt-storage/aleksey/fashion_mnist_clf/clf.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: sagemaker-demo-2019-01-16-23-29-01-518\n",
      "INFO:sagemaker:Creating endpoint with name sagemaker-demo-2019-01-16-23-24-48-787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.predictor import csv_serializer\n",
    "predictor = clf.deploy(1, 'ml.m4.xlarge', serializer=csv_serializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This fails because it lacks an authentication token.\n",
    "# It might be possible to reconstruct the actual POST request being made.\n",
    "# predictor.sagemaker_session.boto_session.get_credentials().token\n",
    "# But the AWS docs are unclear about what name this hearder has.\n",
    "\n",
    "# !curl -X \"POST\" -H \"Content-Type: text/csv\" -d @health-check-data.csv URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv(\"./fashion-mnist_train.csv\").head().iloc[:, 1:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'4,\\n9,\\n4,\\n0,\\n3'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.delete_endpoint(predictor.endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploy a pre-trained model artifact as an endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = sage.estimator.Estimator(image, role, 1, 'ml.c4.2xlarge',\n",
    "                               output_path=\"s3://alpha-quilt-storage/junk\", \n",
    "                               sagemaker_session=sess).create_model()\n",
    "predictor = clf.deploy(1, 'ml.c4.2xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.delete_endpoint(predictor.endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use a model artifact to perform a batch prediction run\n",
    "\n",
    "In order to perform a batch transform you must have a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = sage.estimator.Estimator(image, role, 1, 'ml.c4.2xlarge',\n",
    "                               output_path=\"s3://alpha-quilt-storage/junk\", \n",
    "                               sagemaker_session=sess).create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = sagemaker.transformer.Transformer(\n",
    "    base_transform_job_name='Batch-Transform',\n",
    "    model_name='sagemaker-demo-2019-01-17-02-00-21-619',  # take this from a past training session\n",
    "    instance_count=1,\n",
    "    instance_type='ml.c4.xlarge',\n",
    "    output_path='s3://alpha-quilt-storage/junk',\n",
    "    sagemaker_session=sess\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the job\n",
    "# note: this will fail because the data is not quite in the right input format\n",
    "# but it gets the idea across\n",
    "transformer.transform(\n",
    "    's3://alpha-quilt-storage/aleksey/fashion_mnist/fashion-mnist_train.csv', \n",
    "    content_type='text/csv', \n",
    "    split_type='Line'\n",
    ")\n",
    "\n",
    "# wait until transform job is completed\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: test that this code works\n",
    "import boto3\n",
    "s3_client = boto3.resource('s3')\n",
    "s3_client.download_file(bucket, 'kmeans_batch_example/output/valid-data.csv.out', 'valid-result')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
